use std::path::{Path, PathBuf};
use std::process::{Command, Stdio};
use std::io::{BufRead, BufReader};
use std::sync::mpsc::Sender;
use anyhow::{Result, anyhow};
use crate::app_state::{WhisperModel, WhisperLanguage, ProgressMessage};

/// ä½¿ç”¨VAD pythonè„šæœ¬è¿›è¡Œè¯†åˆ«
pub fn recognize_with_vad(
    audio_path: &Path,
    model: WhisperModel,
    language: &WhisperLanguage,
    custom_language: &str,
    tx: Sender<ProgressMessage>,
) -> Result<PathBuf> {
    // è·å–è¯­è¨€ä»£ç 
    let lang_code = language.to_code(custom_language)
        .ok_or_else(|| anyhow!("Language not specified"))?;
    
    // Pythonè„šæœ¬è·¯å¾„
    let script_path = get_vad_script_path()?;
    
    // è¾“å‡ºSRTè·¯å¾„
    let output_srt = audio_path.with_extension("srt");
    
    println!("ğŸ” ä½¿ç”¨VADæ¨¡å¼è¯†åˆ«...");
    println!("   éŸ³é¢‘: {:?}", audio_path);
    println!("   æ¨¡å‹: {}", model.as_str());
    println!("   è¯­è¨€: {}", lang_code);
    
    // å‘é€å¼€å§‹æ¶ˆæ¯
    let _ = tx.send(ProgressMessage::RealtimeOutput(
        "å¼€å§‹VADè¯­éŸ³æ£€æµ‹...".to_string()
    ));
    
    // æ„å»ºå‘½ä»¤
    let mut cmd = Command::new("python3");
    cmd.arg(&script_path)
       .arg(audio_path)
       .arg("--language")
       .arg(lang_code)
       .arg("--model")
       .arg(model.as_str())
       .stdout(Stdio::piped())
       .stderr(Stdio::piped());
    
    // å¯åŠ¨è¿›ç¨‹
    let mut child = cmd.spawn()?;
    
    // è¯»å–è¾“å‡º
    if let Some(stdout) = child.stdout.take() {
        let reader = BufReader::new(stdout);
        for line in reader.lines() {
            if let Ok(line) = line {
                println!("   VAD: {}", line);
                // å‘é€å®æ—¶è¾“å‡º
                let _ = tx.send(ProgressMessage::RealtimeOutput(line));
            }
        }
    }
    
    // ç­‰å¾…å®Œæˆ
    let status = child.wait()?;
    
    if !status.success() {
        return Err(anyhow!("VADè¯†åˆ«å¤±è´¥"));
    }
    
    // æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if !output_srt.exists() {
        return Err(anyhow!("SRTæ–‡ä»¶æœªç”Ÿæˆ"));
    }
    
    println!("âœ… VADè¯†åˆ«å®Œæˆ: {:?}", output_srt);
    
    Ok(output_srt)
}

/// è·å–VADè„šæœ¬è·¯å¾„
fn get_vad_script_path() -> Result<PathBuf> {
    // å°è¯•å¤šä¸ªå¯èƒ½çš„ä½ç½®
    let possible_paths = vec![
        PathBuf::from("scripts/vad_transcribe_continuous.py"),
        PathBuf::from("../scripts/vad_transcribe_continuous.py"),
        PathBuf::from("./vad_transcribe_continuous.py"),
    ];
    
    for path in possible_paths {
        if path.exists() {
            return Ok(path);
        }
    }
    
    Err(anyhow!("æ‰¾ä¸åˆ°VADè„šæœ¬æ–‡ä»¶"))
}

/// ä½¿ç”¨VADæ¨¡å¼è¯†åˆ«å•ä¸ªç‰‡æ®µï¼ˆç”¨äºManual Cutï¼‰
pub fn recognize_segment_with_vad(
    audio_path: &Path,
    start_time: f64,
    _end_time: f64,
    model: WhisperModel,
    language: &WhisperLanguage,
    custom_language: &str,
    tx: Sender<ProgressMessage>,
) -> Result<Vec<crate::subtitle::SubtitleEntry>> {
    // å…ˆç”¨VADè¯†åˆ«æ•´ä¸ªç‰‡æ®µ
    let srt_path = recognize_with_vad(audio_path, model, language, custom_language, tx)?;
    
    // è§£æSRTæ–‡ä»¶
    let mut subtitles = crate::subtitle::parse_srt_file(&srt_path)?;
    
    // è°ƒæ•´æ—¶é—´åç§»ï¼ˆå› ä¸ºæ˜¯ä»start_timeå¼€å§‹çš„ï¼‰
    for subtitle in &mut subtitles {
        subtitle.start_time += start_time;
        subtitle.end_time += start_time;
    }
    
    Ok(subtitles)
}

